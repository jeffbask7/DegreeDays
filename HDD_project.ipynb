{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDD Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data files needed to be converted to csv files for importation to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #convert raw data file into csv\n",
    "newrow = []\n",
    "\n",
    "with open('geo/countyzones_split.csv', 'w') as f:\n",
    "    with open('geo/countyzones_fixed.csv', 'r') as data:\n",
    "\n",
    "        for line in data:\n",
    "\n",
    "            lineParts = line[:2] + \",\" + line[2:]\n",
    "            newrow = lineParts\n",
    "            f.write(newrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to combine county data ID with States and population. Final output combined County Data including State, County "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine population data with us counties. Limit to US States\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "convCSV = {}\n",
    "\n",
    "#read csvs\n",
    "CountyPops = pd.read_csv('output/newcountycsv.csv')\n",
    "combinedZones = pd.read_csv('geo/countyzones_fixed.csv', sep=',', usecols=[0, 1])\n",
    "stateCountyZones = pd.read_csv('geo/countyzones_split.csv', sep=',', usecols=[0, 1, 2])\n",
    "StateCodes = pd.read_csv('geo/state_codes_fixed_conus.csv')\n",
    "\n",
    "mergedCodes = stateCountyZones.merge(StateCodes, left_on='stateID', right_on='StateId')\n",
    "\n",
    "mergedCodes['states_lower']=(mergedCodes['StateName'].str.lower()).str.strip()\n",
    "CountyPops['states_lower2']= (CountyPops['State'].str.lower()).str.strip()\n",
    "mergedCodes['counties_lower']=(mergedCodes['CountyName'].str.lower()).str.strip()\n",
    "CountyPops['counties_lower2']=(CountyPops['countyName'].str.lower()).str.strip()\n",
    "\n",
    "mergedPop = mergedCodes.merge(CountyPops, how='left', left_on=['counties_lower', 'states_lower'], right_on=['counties_lower2', 'states_lower2'])\n",
    "final_df = pd.DataFrame(mergedPop, columns=['stateID', 'CountyID', 'states_lower', 'counties_lower', 'Pop'])\n",
    "\n",
    "final_df = final_df[final_df.stateID < 57] # limits csv output to US States and exclude territories\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "final_df.to_csv('combinedCountyData_conus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Centerpoint for each U.S. County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find center point of counties and merge with list. output final county data csv\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, MultiPolygon, shape\n",
    "\n",
    "#initialize lists\n",
    "result = []\n",
    "gefsResult = []\n",
    "dfCC = []\n",
    "badCounties = []\n",
    "  \n",
    "# Opening county data JSON file\n",
    "f = open('geo/gz_2010_is_050_00_20m2.json')\n",
    "\n",
    "# reading gefs csv into dataframe\n",
    "CountyData = pd.read_csv('combinedCountyData_conus.csv', dtype=str)\n",
    "dfCountyData = pd.DataFrame(CountyData)\n",
    "\n",
    "#convert state and county ids to strings with 2 and 3 chars respectively\n",
    "CountyData['stateID'] = CountyData['stateID'].str.zfill(2)\n",
    "CountyData['CountyID'] = CountyData['CountyID'].str.zfill(3)\n",
    "\n",
    "data = json.load(f) #create json object\n",
    "  \n",
    "# Iterating through the json list\n",
    "i=0\n",
    "for Feature in data['features']:\n",
    "    geoID = Feature['properties']['GEO_ID']\n",
    "    countyID = Feature['properties']['COUNTY']\n",
    "    stateID = Feature['properties']['STATE']\n",
    "    countyName = Feature['properties']['NAME']\n",
    "    coordsBoundary = (Feature['geometry']['coordinates'])\n",
    "    coordsBoundaryFlat = [val for sublist in coordsBoundary for val in sublist]\n",
    "    #find center of county polygon\n",
    "    try:  \n",
    "        centerCoord = Polygon(coordsBoundaryFlat).centroid\n",
    "        lon = centerCoord.x\n",
    "        lat = centerCoord.y\n",
    "        newLine = [stateID, countyID, countyName, lat, lon]\n",
    "        result.append(newLine)\n",
    "    #if no valid polygon try multi-polygon solution    \n",
    "    except: \n",
    "        try:\n",
    "            polys = [x.buffer(0) for x in shape(Feature['geometry']).buffer(0).geoms] #extract shapely polygons into list of polygons\n",
    "            centerCoord = MultiPolygon([*polys]).centroid # find center of polygons in list\n",
    "            lon = centerCoord.x\n",
    "            lat = centerCoord.y\n",
    "            newLine = [stateID, countyID, countyName, lat, lon]\n",
    "            result.append(newLine)\n",
    "        # if no valid polygon or multipolygon geomtery found skip line and print error\n",
    "        except:\n",
    "            badLine = [stateID, countyID, countyName]\n",
    "            print(f'shapely doesnt like {badLine}')\n",
    "            badCounties.append(badLine)\n",
    "    i = i + 1\n",
    "  \n",
    "# Closing file\n",
    "f.close()\n",
    "\n",
    "#creating county data dataframe with centroid lat lon\n",
    "dfResults = pd.DataFrame(result, columns=['stateID','countyID','county','lat','lon'])\n",
    "dfBadResults = pd.DataFrame(badCounties)\n",
    "\n",
    "dfResults = pd.merge(dfResults, CountyData, how='left', left_on=['stateID', 'countyID'], right_on=['stateID', 'CountyID'] )\n",
    "dfResults['Pop'] = (dfResults['Pop']).str.replace(',','')\n",
    "dfResults['Pop'] = (dfResults['Pop']).truncate()\n",
    "dfResults = dfResults.drop(columns=['CountyID'])\n",
    "\n",
    "dfResults = dfResults[(dfResults.stateID).astype(int) < 57]\n",
    "\n",
    "print(dfResults)\n",
    "dfResults.to_csv('county_centroid_conus.csv', index=False) # final result\n",
    "\n",
    "dfBadResults.to_csv('output/badCounties.csv', index=False) #csv for counties with failed center point calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign GFS Grid Index value to Counties. Grid index references closest grid point coords to county center coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and join GFS GridIndex to County Data\n",
    "\n",
    "from scipy.spatial.distance import cdist \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#read model grid csv and county data csv into dataframes  \n",
    "gefsTemp = pd.read_csv('geo/gfstemp12_18.csv')\n",
    "CountyData = pd.read_csv('county_centroid_conus.csv')\n",
    "\n",
    "# creating gefs dataframe with appropriate columns\n",
    "dfGefsTemp = pd.DataFrame(gefsTemp, columns=['value','latitude','longitude'])\n",
    "dfGefsTemp.insert(0, 'gridIndex', range(1, 1 + len(dfGefsTemp)))\n",
    "dfTempIndex = pd.DataFrame(dfGefsTemp, columns=['gridIndex', 'value'])\n",
    "\n",
    "#find nearest grid point to county center\n",
    "distArray = cdist(CountyData[['lat','lon']], dfGefsTemp[['latitude', 'longitude']])\n",
    "gridIndex =  [dfGefsTemp[\"gridIndex\"].iloc[np.argmin(x)] for x in distArray]\n",
    "\n",
    "# insert gefs temp value column into county dataframe\n",
    "CountyData['gridIndex'] = gridIndex\n",
    "#left join gfs gridIndex to county list\n",
    "CountyData = CountyData.merge(dfTempIndex, how='left', left_on=['gridIndex'], right_on=['gridIndex'])\n",
    "\n",
    "CountyData['Pop'] = (CountyData['Pop']).truncate()   #truncate population column\n",
    "\n",
    "CountyData.to_csv('county_grid_gfs.csv', index=False)   #create csv file from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate degree hours from gfs csv output and assign grid point index to data points. Output is in csv format for manual add to analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join data from model grid points \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TempIndexList = []\n",
    "inputFiles = []\n",
    "runtime = 12 #label model runtime of gfs output\n",
    "rundate = '112522' #lablek model runtime of gfs output\n",
    "\n",
    "#create list of input files from gfs directory\n",
    "def createInputFiles():\n",
    "    i = 18\n",
    "    while i < 43:\n",
    "        istr = str(i).zfill(3)\n",
    "        filename = f'gfs/gfs-{runtime}-fcsthr{istr}-{rundate}.csv'\n",
    "        inputFiles.append(filename)\n",
    "        i = i + 1\n",
    "    return inputFiles\n",
    "\n",
    "inputFiles = createInputFiles()\n",
    "\n",
    "def gfsData(inputFiles):\n",
    "    gridTempFrames = []\n",
    "    TempIndexList = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    l = 0\n",
    "\n",
    "    def createGridFrame(inputfile, x):\n",
    "        gridTempFrame = pd.read_csv(inputfile)\n",
    "        gridTempFrames.append(gridTempFrame)\n",
    "        return gridTempFrames\n",
    "\n",
    "    while i < len(inputFiles):\n",
    "        createGridFrame(inputFiles[i], i)\n",
    "        i = i + 1\n",
    "\n",
    "    def createIndex(df, y):\n",
    "        df.insert(0, 'gridIndex', range(1, 1 + len(df)))\n",
    "        dfTempIndex = pd.DataFrame(df, columns=['gridIndex', 'value', 'datetime'])\n",
    "        dfTempIndex = dfTempIndex.rename(columns = {'value':f'tempK'})\n",
    "        dfTempIndex = dfTempIndex.rename(columns = {'datetime':f'datetime'})\n",
    "        TempIndexList.append(dfTempIndex)\n",
    "        return TempIndexList\n",
    "\n",
    "    while j < len(gridTempFrames):\n",
    "        createIndex(gridTempFrames[j], j)\n",
    "        j = j + 1\n",
    "    \n",
    "    #calculate heating and cooling degree hours\n",
    "    def calc_dd(TempIndexList, x):\n",
    "        TempIndexList[x]['hdd'] = (65 - (9/5*(TempIndexList[x]['tempK'] - 273) + 32).round())  \n",
    "        TempIndexList[x]['cdd'] = ((9/5*(TempIndexList[x]['tempK'] - 273) + 32).round() - 65)\n",
    "        TempIndexList[x].loc[TempIndexList[x][f'hdd'] < 0, 'hdd'] = 0\n",
    "        TempIndexList.loc[TempIndexList['cdd'] < 0, 'cdd'] = 0\n",
    "        TempIndexList[x].to_csv(f'gfs_output/gfs{x}.csv', index=False)\n",
    "        return TempIndexList\n",
    "\n",
    "    while l < len(TempIndexList):\n",
    "        TempIndexList = calc_dd(TempIndexList, l)\n",
    "        l = l + 1\n",
    "    \n",
    "    print(TempIndexList)\n",
    "    return TempIndexList\n",
    "\n",
    "TempIndexList = gfsData(inputFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert GFS output for each forecast hour as table into RDS Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "TempIndexList = []\n",
    "inputFiles = []\n",
    "runtime = 12\n",
    "rundate = '112522'\n",
    "\n",
    "def createInputFiles():\n",
    "    i = 18\n",
    "    while i < 43:\n",
    "        istr = str(i).zfill(3)\n",
    "        filename = f'gfs/gfs-{runtime}-fcsthr{istr}-{rundate}.csv'\n",
    "        inputFiles.append(filename)\n",
    "        i = i + 1\n",
    "    return inputFiles\n",
    "\n",
    "inputFiles = createInputFiles()\n",
    "\n",
    "def gfsData(inputFiles):\n",
    "    gridTempFrames = []\n",
    "    TempIndexList = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    l = 0\n",
    "\n",
    "    def createGridFrame(inputfile, x):\n",
    "        gridTempFrame = pd.read_csv(inputfile)\n",
    "        gridTempFrames.append(gridTempFrame)\n",
    "        return gridTempFrames\n",
    "\n",
    "    while i < len(inputFiles):\n",
    "        createGridFrame(inputFiles[i], i)\n",
    "        i = i + 1\n",
    "\n",
    "    def createIndex(df, y):\n",
    "        df.insert(0, 'gridIndex', range(1, 1 + len(df)))\n",
    "        dfTempIndex = pd.DataFrame(df, columns=['gridIndex', 'value', 'datetime'])\n",
    "        dfTempIndex = dfTempIndex.rename(columns = {'value':f'tempK{y}'})\n",
    "        dfTempIndex = dfTempIndex.rename(columns = {'datetime':f'datetime{y}'})\n",
    "        TempIndexList.append(dfTempIndex)\n",
    "        return TempIndexList\n",
    "\n",
    "    while j < len(gridTempFrames):\n",
    "        createIndex(gridTempFrames[j], j)\n",
    "        j = j + 1\n",
    "    \n",
    "    #calculate heating and cooling degree hours\n",
    "    def calc_dd(TempIndexList, x):\n",
    "        TempIndexList[x][f'hdd{x}'] = (65 - (9/5*(TempIndexList[x][f'tempK{x}'] - 273) + 32).round())  \n",
    "        #CountyData['cdd'] = ((9/5*(CountyData[f'tempK{x}'] - 273) + 32).round() - 65)\n",
    "        TempIndexList[x].loc[TempIndexList[x][f'hdd{x}'] < 0, f'hdd{x}'] = 0\n",
    "        #CountyData.loc[CountyData['cdd'] < 0, 'cdd'] = 0\n",
    "        return TempIndexList\n",
    "\n",
    "    while l < len(TempIndexList):\n",
    "        TempIndexList = calc_dd(TempIndexList, l)\n",
    "        l = l + 1\n",
    "    \n",
    "    print(TempIndexList)\n",
    "    return TempIndexList\n",
    "\n",
    "TempIndexList = gfsData(inputFiles)\n",
    "\n",
    "#Connect to RDS Postgres\n",
    "\n",
    "ENDPOINT=\"{rds endpoint}\"\n",
    "PORT=\"{port}\"\n",
    "USER=\"{user}\"\n",
    "REGION=\"{region}\"\n",
    "DBNAME=\"{db name}\"\n",
    "\n",
    "session = boto3.Session(profile_name='dev')\n",
    "client = session.client('rds', region_name='us-west-2')\n",
    "\n",
    "token = client.generate_db_auth_token(DBHostname=ENDPOINT, Port=PORT, DBUsername=USER, Region=REGION)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=ENDPOINT, port=PORT, database=DBNAME, user=USER, password=token, sslrootcert=\"SSLCERTIFICATE\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SELECT now()\"\"\")\n",
    "    query_results = cur.fetchall()\n",
    "    print(query_results)\n",
    "except Exception as e:\n",
    "    print(\"Database connection failed due to {}\".format(e))     \n",
    "\n",
    "#create tables and insert dataframes from tempindexlist\n",
    "z = 18\n",
    "w = 0\n",
    "for df in TempIndexList:\n",
    "    table = f'public.gfs{z}'\n",
    "    tableName = f'gfs{z}'\n",
    "    command = (f\"\"\"create table {table}(\n",
    "    gridIndex int,\n",
    "    tempK{w} numeric,\n",
    "    datetime{w} timestamp,\n",
    "    hdd{w} numeric)\"\"\") \n",
    "    \n",
    "\n",
    "\n",
    "    cur.execute(command)\n",
    "\n",
    "    def execute_values(conn, df, table):\n",
    "        \"\"\"\n",
    "        Using psycopg2.extras.execute_values() to insert the dataframe\n",
    "        \"\"\"\n",
    "        # Create a list of tupples from the dataframe values\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "        # Comma-separated dataframe columns\n",
    "        cols = ','.join(list(df.columns))\n",
    "        # SQL quert to execute\n",
    "        query  = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            extras.execute_values(cursor, query, tuples)\n",
    "            conn.commit()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(\"Error: %s\" % error)\n",
    "            conn.rollback()\n",
    "            cursor.close()\n",
    "            return 1\n",
    "        print(\"execute_values() done\")\n",
    "        cursor.close()\n",
    "\n",
    "    execute_values(conn, df, tableName)\n",
    "    # commit the changes\n",
    "    conn.commit()\n",
    "    z = z + 1\n",
    "    w = w + 1\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting county data table with gfs gridIndex values to RDS. New gridIndex needs to be created for different model data or for any model boundary modification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('county_grid_gfs.csv', usecols=['stateID','countyID','county','lat','lon','states_lower','counties_lower','Pop','gridIndex'])\n",
    "\n",
    "commandCreateCountyDataTable = (\"\"\"create table public.countyData_gfs(\n",
    "stateID varchar,\n",
    "countyID varchar,\n",
    "county varchar,\n",
    "lat float,\n",
    "lon float,\n",
    "states_lower varchar,\n",
    "counties_lower varchar,\n",
    "Pop numeric,\n",
    "gridIndex numeric\n",
    ")\"\"\") \n",
    "\n",
    "cur.execute(commandCreateCountyDataTable)\n",
    "\n",
    "def execute_values(conn, df, table):\n",
    "    \"\"\"\n",
    "    Using psycopg2.extras.execute_values() to insert the dataframe\n",
    "    \"\"\"\n",
    "    # Create a list of tupples from the dataframe values\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(list(df.columns))\n",
    "    # SQL quert to execute\n",
    "    query  = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    print(\"execute_values() done\")\n",
    "    cursor.close()\n",
    "\n",
    "execute_values(conn, df, 'countyData_gfs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
